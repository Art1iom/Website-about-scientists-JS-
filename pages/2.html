<!doctype html>
<html>
<head>
<title> Ральф Хартли </title>
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" >
<link rel="stylesheet" type="text/css" href="../css/1.css">
<meta charset="utf-8">
</head>
<body>
	<a name="TOP"></a>
<header> Ральф Винтон Лайон Хартли </header>
	<nav>
		<h3> Меню сайта </h3>
  <ul class="topmenu">
    <li><a href="../index.html">Теория информации</a></li>
    <li><a href="">Учёные</a>
       <ul class="submenu">
    <li><a href="1.html">Клод Шеннон</a></li>
     <li><a href="2.html">Ральф Винтон Лайон Хартли</a></li>
     </ul>
    </li>
    <li><a href="3.html">Глоссарий</a></li>
  </ul>
</nav>
	<div> 
	<table border="1" height="130">
	<tr>
		<td> Дата Рождения </td>
		<td> 30 Ноября 1888 г. </td>
	</tr>
	<tr>
		<td> Дата смерти </td>
		<td> 1 Мая 1970 г. (81 год) </td>
	</tr>
	<tr>
		<td> Научная сфера </td>
		<td> Электротехника </td>
	</tr>
	</table> 
	<p > 
		<dfn>Ральф Винтон Лайон Хартли</dfn> — американский учёный-электронщик. <br>Он предложил генератор Хартли, преобразование Хартли и сделал вклад в теорию информации, введя в 1928 году логарифмическую меру информации, которая называется хартлиевским количеством информации или просто мерой Хартли.
	</p>
	<hr>
	<figure class="img">
		<img  src="../pic/Ральф Хартли.jpg" >
			<br>
		<figcaption> Ральф Винтон Лайон Хартли </figcaption>
	</figure>
		<h3 class="ci"> Цитата </h3>
		<q>
 «Хотя частотные отношения в электрических связях интересны сами по себе, их обсуждение в этой ситуации едва ли будет оправдано, если мы не сможем вывести из них достаточно общие практические применения к разработке систем связи. То, что я надеюсь достичь в этом направлении, должно установить количественную меру, посредством которой могут быть сравнены мощности различных систем передачи информации» – [1, стр.535] </q>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <br>
 <figure class="R" > <img src="../pic/Ralph.jpg" > <br> <figcaption> Ральф Винтон Лайон Хартли </figcaption></figure>
<p><mark>Хартли был пионером в области Информационной Теории.</mark> Он ввёл понятие <dfn>«информации»</dfn> (энтропии) как случайной переменной и был первым, кто попытался определить «<dfn>меру информации</dfn>». Хартли развивал понятие информации, основанной на «физическом как противопоставлено с психологическими рассмотрениями» для использования в изучении электронных коммуникаций. Фактически, Хартли соответственно определяет это основное понятие. Вместо этого он обращается к «точности ... информации» и «количеству информации».</p>
<h1>Формула Хартли</h1>
 <p>Интуитивно ясно, что если имеется N равновероятных исходов, то количество информации, которое мы получаем при реализации одного из них, составляет log2N. Докажем это более строго.
Изменение для набора из N различных символов можно составить Nm различных комбинаций (“слов”) длины m (это утверждение легко доказывается по индукции, и известно из комбинаторики).
Существует такое k∈N – количество необходимых для кодирования этого слова двоичных знаков – что</p>
<br>
<p class="c"><b>2k&lt;Nm≤2k+1</b></p>
<br>
<h2 class="h">Логарифмируя неравенства, получаем:</h2>
 <br>
<p class="c"><b>k&lt;mlog2N≤k+1</b></p>
<br>
<pre>                                                                               <big>или</big></pre>
<br>
<p class="c"><b>km&lt;log2N≤km+1m</b></p>
<br>

<p>Таким образом, среднее количество информации, приходящееся на один символ km отличается от log2N не более, чем на 1m.
Тогда при оптимальном кодировании информации (которое при неограниченном m достижимо с произвольной точностью), на один символ приходится log2N бит информации (или, что то же, один символ кодируется в среднем log2N двоичными символами) не изменяется.</p>
<br>
<p class="c"><b>Выражение: H=log2N</b>,</p>
<br>
<p>где H – количество информации в символе N-значного алфавита,<mark> называется Формулой Хартли.</mark></p>

 <p class="pp"><b>Формула Хартли</b> находит много различных применений. Одно из наиболее интересных заключается в оценке классов сложности вычислительных задач.</p>
 	
 	<h2><strong>Рассмотрим на примере алгоритмов сортировки:</strong></h2></p>	
 		
 		<p>С точки зрения теории информации, сортировка представляет собой выбор из всех возможных перестановок массива одной такой, которая удовлетворяет критерию сортировки.
Массив длинны N имеет N! различных перестановок (это утверждение легко доказывается по индукции, и известно из комбинаторики).
Операция сравнения двух элементов по сути несет один бит информации (либо первый больше второго, либо меньше или равен), а <mark>по формуле Хартли количество информации</mark>, соответствующее выбору одного из N! вариантов H=log2(N!), то есть, потребуется по крайней мере log2(N!) операций сравнения.
Для больших N можно использовать формулу Стирлинга для оценки факториала.</p>
<h2>Награды и звания</h2>
<p class="pp">Хартли награждён премиями за отличия в области науки, этот учёный состоял в американской Ассоциации Продвижения Науки. Хартли принадлежат больше чем <br><b><mark>70 патентов (изобретений).</mark></b></p>

	<footer><a href="#TOP">Наверх</a></footer>
	<footer><b> Сайт разработал Глотов А.И.</b></footer>
</div>
</body>
</html>